{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Control Expirements\n",
    "\n",
    "Main notebook for using a base model (at this point it is BART) for generating with controllable complexity. List of things to try: \n",
    "\n",
    "- ```GeDi```: Using the discriminator guided generation. Dataset is arxiv vs. science news. \n",
    "- ```DExperts```: Using a pretrained model as an expert and anti experts. The two pretrained models are one of arxiv text, one on science news text. \n",
    "- ```Rerankers```: An SVM trained on same DS as above (arxiv and science news). \n",
    "- ```PPLM```: Not included here, run seperately based on the HUF code (https://github.com/huggingface/transformers/tree/main/examples/research_projects/pplm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "     AutoTokenizer,\n",
    "     AutoModelForSeq2SeqLM,\n",
    "     AutoConfig, \n",
    "     LogitsProcessorList,\n",
    "     MinLengthLogitsProcessor,\n",
    "     TopKLogitsWarper,\n",
    "     NoBadWordsLogitsProcessor,\n",
    "     TemperatureLogitsWarper,\n",
    "     BeamSearchScorer,\n",
    "     LogitsProcessor,\n",
    "     NoRepeatNGramLogitsProcessor,\n",
    "     LogitsProcessorList,\n",
    ")\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from importlib import reload  \n",
    "from joblib import dump, load\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import datasets\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import spacy \n",
    "import scispacy\n",
    "from tqdm import tqdm\n",
    "from spacy.pipeline import Sentencizer\n",
    "import re\n",
    "from typing import Iterable, List\n",
    "from transformers import BertTokenizer, glue_convert_examples_to_features, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "sys.path.append('/lib')\n",
    "\n",
    "import utils\n",
    "import jargon_utils\n",
    "\n",
    "MODEL_DIR = ''\n",
    "RESOURCE_DIR = ''\n",
    "GEDI_PATH = ''\n",
    "DATA_DIR = ''\n",
    "\n",
    "sys.path.append(GEDI_PATH)\n",
    "sys.path.append(RESOURCE_DIR)\n",
    "\n",
    "import GeDi\n",
    "from GeDi import generate_GeDi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For now, just use the medq wiki dataset (which is what the base model is finetuned oned), but might want to also try the sci defs or the simple and normal wiki paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Dataset reloading \n",
    "df_wiki_medq_strat_train = pd.read_csv('{}/model_data/medquad_wikipedia_k_means_with_sd_train.csv'.format(DATA_DIR))\n",
    "df_wiki_medq_strat_dev = pd.read_csv('{}/model_data/medquad_wikipedia_k_means_with_sd_dev.csv'.format(DATA_DIR))\n",
    "df_wiki_medq_strat_test = pd.read_csv('{}/model_data/medquad_wikipedia_k_means_with_sd_test.csv'.format(DATA_DIR))\n",
    "\n",
    "#### for testing without running through the whole df\n",
    "df_test = df_wiki_medq_strat_test\n",
    "\n",
    "#### for just running the human eval (and then not human eval)\n",
    "selected_questions = pd.read_csv('{}/model_data/human_eval_test_questions.txt'.format(DATA_DIR))\n",
    "df_test_human_eval = df_test[df_test['question'].isin(selected_questions['question'])]\n",
    "df_test_not_human_eval = df_test[~df_test['question'].isin(selected_questions['question'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUMBER = '' \n",
    "CUDA_DEVICE = '0'\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_NUMBER\n",
    "\n",
    "DEVICE = 'cuda:0' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Types \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is similar to the function from the definition modelling notebook but simplified to ignore gpt2 and added parts fo ranking scores\n",
    "def generate_answers(df, model, tokenizer, model_type,\n",
    "                     num_return_sequences=1,\n",
    "                     num_beams=5,\n",
    "                     max_length=64,\n",
    "                     min_length=8,\n",
    "                     early_stopping=True,\n",
    "                     temperature=1,\n",
    "                     do_sample=True,\n",
    "                     top_k=50,\n",
    "                     top_p=0.9,\n",
    "                     max_input_length=1024,\n",
    "                     no_repeat_ngram_size=3,\n",
    "                     device=None):\n",
    "    \n",
    "    answer_df_lists = []\n",
    "    for i, r in tqdm(df.iterrows()):\n",
    "        \n",
    "        row = r.to_dict()\n",
    "                \n",
    "        ### make input\n",
    "        q_doc = row['q_s2orc_doc']\n",
    "        inputs = tokenizer(q_doc, return_tensors='pt', max_length=max_input_length, truncation=True)\n",
    "        \n",
    "        \n",
    "        outputs = model.generate(**inputs.to(device),\n",
    "                                 decoder_start_token_id=tokenizer.bos_token_id,\n",
    "                                 num_return_sequences=num_return_sequences,\n",
    "                                 num_beams=num_beams, \n",
    "                                 min_length=min_length,\n",
    "                                 max_length=max_length, \n",
    "                                 early_stopping=early_stopping, \n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=do_sample,\n",
    "                                 top_k=top_k,\n",
    "                                 top_p=top_p,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                 output_scores=True,\n",
    "                                 return_dict_in_generate=True)\n",
    "\n",
    "        # save all the answers with their associated scores in a df\n",
    "        answers = [tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in outputs[0]]\n",
    "        df_answers = pd.DataFrame(zip(answers, outputs['sequences_scores'].tolist()), columns=['response', 'scores'])\n",
    "        df_answers['model-type'] = model_type\n",
    "        \n",
    "        # save information about the question\n",
    "        df_answers['question'] = row['question']\n",
    "        df_answers['category'] = row['category']\n",
    "        df_answers['first_sentence'] = row['first_sentence']\n",
    "    \n",
    "        # append the df\n",
    "        answer_df_lists.append(df_answers)\n",
    "        \n",
    "    return pd.concat(answer_df_lists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranker\n",
    "\n",
    "Taking a slightly different approach from last time, where we will generate on the base model then score seperately here. \n",
    "\n",
    "For scoring, do so grouped by question, because I think the svm decision function matters for that and maybe the bart one does too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helpful functions\n",
    "\n",
    "def get_svm_score(answers, clf, response_col):\n",
    "    feature_cols = ['avl_occ', 'function_words_prop', 'te_oov', 'response_gpt_ppl_score', 'word_count']\n",
    "    answers = jargon_utils.make_jargon_features(answers, response_col)\n",
    "    answers['predictions'] = clf.decision_function(answers[feature_cols])\n",
    "    return answers\n",
    "\n",
    "\n",
    "def get_bert_logits(answers, bert_tokenizer, bert_ranker_model, prefix='', device=None):\n",
    "    answers['input_ids'] = [bert_tokenizer(a, return_tensors=\"pt\") for a in answers['response']]\n",
    "    answers[prefix+'bert_logits'] = [bert_ranker_model(**inputs.to(device)).logits.cpu().detach().numpy()[0] for inputs in answers['input_ids']]\n",
    "    answers[[prefix+'bert_logits_news', prefix+'bert_logits_academic']] = answers[prefix+'bert_logits'].apply(pd.Series)\n",
    "    \n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get generator and rankers\n",
    "\n",
    "### Base Model\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"{}/bart_medq_wiki_s2orc/default\".format(MODEL_DIR))\n",
    "bart_model = AutoModelForSeq2SeqLM.from_pretrained(\"{}/bart_medq_wiki_s2orc/default\".format(MODEL_DIR)).to(DEVICE)\n",
    "\n",
    "_ = bart_model.eval()\n",
    "\n",
    "\n",
    "#### SVM \n",
    "ranker_path = ''\n",
    "feature_cols = ['avl_occ', 'function_words_prop', 'te_oov', 'response_gpt_ppl_score', 'word_count']\n",
    "arxiv_sci_train_clf = load('{}/svm_train_arxiv.joblib'.format(ranker_path)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating some answers based on the base model\n",
    "\n",
    "### Because of cuda issues, loop through 20 times (*5 per time) for 100 answers each\n",
    "all_bart_test_answers = []\n",
    "for _ in range(20):\n",
    "    bart_test_answers = generate_answers(df=df_test, num_return_sequences=5, model=bart_model, tokenizer=bart_tokenizer, model_type='bart-base', use_decoder_prefix=False, device=DEVICE)\n",
    "    all_bart_test_answers.append(bart_test_answers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the bart_dev_answers var because that is what is used below\n",
    "bart_test_answers = pd.concat(all_bart_test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scoring the answers\n",
    "\n",
    "bart_test_answers_svm = bart_test_answers.groupby(['question']).apply(lambda g: get_svm_score(g, arxiv_sci_train_clf, response_col='response')).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### take models off the gpu \n",
    "bart_model=bart_model.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Save everything seperately\n",
    "bart_test_answers_svm.to_csv('{}/bart_test_answers_svm.csv'.format(DATA_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DExperts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### These models are bart-large pretrained on the two datasets (but not finetuned on our task)\n",
    "### rather than the DAPT models, which are pretrained on top of the finetuned bart\n",
    "\n",
    "bart_article_model = AutoModelForSeq2SeqLM.from_pretrained(\"{}/bart_medq_wiki_s2orc/articles\".format(MODEL_DIR)).to(DEVICE)\n",
    "_ = bart_article_model.eval()\n",
    "\n",
    "bart_journal_model = AutoModelForSeq2SeqLM.from_pretrained(\"{}/bart_medq_wiki_s2orc/journals\".format(MODEL_DIR)).to(DEVICE)\n",
    "_ = bart_journal_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly taken from https://huggingface.co/transformers/_modules/transformers/generation_utils.html#GenerationMixin.generate\n",
    "# for encoder-decoder networks like BART \n",
    "def setup_s2s_generation(context, s2s_model, s2s_tokenizer, max_length=50):\n",
    "    inputs = s2s_tokenizer(context, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        \n",
    "    model_kwargs = s2s_model._prepare_encoder_decoder_kwargs_for_generation(inputs['input_ids'], {})\n",
    "\n",
    "    input_ids = s2s_model._prepare_decoder_input_ids_for_generation(\n",
    "        inputs['input_ids'], decoder_start_token_id=s2s_model.config.decoder_start_token_id, bos_token_id=s2s_model.config.bos_token_id\n",
    "    )\n",
    "    \n",
    "    sequence_lengths, unfinished_sequences, cur_len = s2s_model._init_sequence_length_for_generation(\n",
    "        input_ids, max_length\n",
    "    )\n",
    "    \n",
    "    return input_ids, model_kwargs, sequence_lengths, unfinished_sequences\n",
    "\n",
    "\n",
    "def generate_s2s_DEexperts(context,\n",
    "                           model,\n",
    "                           expert_model,\n",
    "                           anti_model,\n",
    "                           tokenizer,\n",
    "                           top_k=50,\n",
    "                           top_p=0.9,\n",
    "                           num_beams=5,\n",
    "                           max_length=64,\n",
    "                           min_length=8,\n",
    "                           no_repeat_ngram_size=3,\n",
    "                           expert_alpha = 2.0,\n",
    "                           early_stopping=True,\n",
    "                           temperature=1.0,\n",
    "                           do_sample=True, # not used, assumed\n",
    "                           device=None,\n",
    "                           num_return_sequences=1):\n",
    "\n",
    "    input_ids, model_kwargs, sequence_lengths, unfinished_sequences = setup_s2s_generation(context, model, tokenizer, max_length)\n",
    "\n",
    "    \n",
    "    # make the beam search scorer\n",
    "    beam_scorer = BeamSearchScorer(\n",
    "                 batch_size=1,\n",
    "                 max_length=max_length,\n",
    "                 num_beams=num_beams,\n",
    "                 device=device,\n",
    "                 length_penalty=model.config.length_penalty,\n",
    "                 do_early_stopping=early_stopping,\n",
    "                 num_beam_hyps_to_keep=num_return_sequences)\n",
    "        \n",
    "        \n",
    "    logits_processors = model._get_logits_processor(\n",
    "            repetition_penalty=None,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            bad_words_ids=None,\n",
    "            min_length=min_length,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            prefix_allowed_tokens_fn=None,\n",
    "            num_beams=num_beams,\n",
    "            num_beam_groups=1,\n",
    "            diversity_penalty=None,\n",
    "        )\n",
    "    \n",
    "\n",
    "    logits_warper = model._get_logits_warper(\n",
    "            top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "        )\n",
    "\n",
    "    eos_token_id = model.config.eos_token_id\n",
    "    pad_token_id = model.config.pad_token_id\n",
    "\n",
    "    \n",
    "    # interleave with `num_beams`\n",
    "    input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
    "        input_ids, expand_size=num_beams, is_encoder_decoder=model.config.is_encoder_decoder, **model_kwargs\n",
    "    )\n",
    "    \n",
    "    # initalize for beam search\n",
    "    batch_size = len(beam_scorer._beam_hyps)\n",
    "    num_beams = beam_scorer.num_beams\n",
    "\n",
    "    batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "    beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "        \n",
    "    \n",
    "\n",
    "    while cur_len < max_length:\n",
    "        # prepare inputs\n",
    "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "        # run forward of model\n",
    "        output = model(**model_inputs)\n",
    "\n",
    "        next_token_logits = output.logits[:, -1, :]\n",
    "        \n",
    "        \n",
    "        # hack from transformers: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`\n",
    "        # cannot be generated both before and after the `F.log_softmax` operation.\n",
    "        next_token_logits = model.adjust_logits_during_generation(\n",
    "            next_token_logits, cur_len=cur_len, max_length=max_length\n",
    "        )\n",
    "        \n",
    "        next_token_scores = F.log_softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # pre-process distribution\n",
    "        next_token_scores = logits_processors(input_ids, next_token_logits)\n",
    "        next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)\n",
    "        next_token_scores = logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "        # get the anti-expert and expert\n",
    "        expert_output = expert_model(**model_inputs)\n",
    "        anti_output = anti_model(**model_inputs)\n",
    "        \n",
    "\n",
    "        next_token_scores = next_token_scores + (expert_alpha * (expert_output.logits[:, -1, :] - anti_output.logits[:, -1, :]))\n",
    "\n",
    "        \n",
    "        # reshape for beam search\n",
    "        vocab_size = next_token_scores.shape[-1]\n",
    "        next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "        probs = F.softmax(next_token_scores, dim=-1)\n",
    "\n",
    "        next_tokens = torch.multinomial(probs, num_samples=(2 * num_beams))\n",
    "        next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n",
    "\n",
    "        next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n",
    "        next_tokens = torch.gather(next_tokens, -1, _indices)\n",
    "\n",
    "        next_indices = next_tokens // vocab_size\n",
    "        next_tokens = next_tokens % vocab_size\n",
    "\n",
    "        # stateless\n",
    "        beam_outputs = beam_scorer.process(\n",
    "            input_ids,\n",
    "            next_token_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "        )\n",
    "        beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "        beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "        beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "        \n",
    "        \n",
    "        input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        cur_len += 1\n",
    "        \n",
    "        # update kwargs\n",
    "        model_kwargs = model._update_model_kwargs_for_generation(\n",
    "                    output, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder\n",
    "        )\n",
    "        \n",
    "        # update sequence length\n",
    "        if eos_token_id is not None:\n",
    "            sequence_lengths, unfinished_sequences = model._update_seq_length_for_generation(\n",
    "                sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id\n",
    "            )\n",
    "\n",
    "\n",
    "        # pretty sure this is early stopping\n",
    "        if unfinished_sequences.max() == 0:\n",
    "            break\n",
    "        \n",
    "\n",
    "        # pretty sure this is early stopping for the beam scorer\n",
    "        if beam_scorer.is_done:\n",
    "            break\n",
    "            \n",
    "    sequence_outputs = beam_scorer.finalize(\n",
    "        input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id\n",
    "    )\n",
    "            \n",
    "    return sequence_outputs[\"sequences\"], model_kwargs, sequence_outputs[\"sequence_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Journal \n",
    "journal_answer_df_lists = []\n",
    "\n",
    "for i, r in tqdm(df_test_not_human_eval.iterrows()):\n",
    "\n",
    "    row = r.to_dict()\n",
    "\n",
    "    ### make input\n",
    "    q_doc = row['q_s2orc_doc']\n",
    "    output_ids, model_kwargs, seq_scores = generate_s2s_DEexperts(context=q_doc, \n",
    "                                                 model=bart_model,\n",
    "                                                 expert_model=bart_journal_model,\n",
    "                                                 anti_model=bart_article_model,\n",
    "                                                 tokenizer=bart_tokenizer,\n",
    "                                                 top_k=50,\n",
    "                                                 top_p=.9,\n",
    "                                                 num_beams=10,\n",
    "                                                 max_length=64,\n",
    "                                                 min_length=8,\n",
    "                                                 no_repeat_ngram_size=3,\n",
    "                                                 expert_alpha = 1.0,\n",
    "                                                 num_return_sequences=10\n",
    "                                                 )\n",
    "\n",
    "    # save all the answers with their associated scores in a df\n",
    "    answers = [bart_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in output_ids]\n",
    "    df_answers = pd.DataFrame(zip(answers, seq_scores), columns=['response', 'scores'])\n",
    "    \n",
    "    df_answers['model-type'] = 'DExpert-journal'\n",
    "\n",
    "    # save information about the question\n",
    "    df_answers['question'] = row['question']\n",
    "    df_answers['category'] = row['category']\n",
    "    df_answers['first_sentence'] = row['first_sentence']\n",
    "\n",
    "    # append the df\n",
    "    journal_answer_df_lists.append(df_answers)\n",
    "\n",
    "        \n",
    "###### News\n",
    "article_answer_df_lists = []\n",
    "\n",
    "for i, r in tqdm(df_test_not_human_eval.iterrows()):\n",
    "\n",
    "    row = r.to_dict()\n",
    "\n",
    "    ### make input\n",
    "    q_doc = row['q_s2orc_doc']\n",
    "    output_ids, model_kwargs, seq_scores = generate_s2s_DEexperts(context=q_doc, \n",
    "                                                 model=bart_model,\n",
    "                                                 expert_model=bart_article_model,\n",
    "                                                 anti_model=bart_journal_model,\n",
    "                                                 tokenizer=bart_tokenizer,\n",
    "                                                 top_k=50,\n",
    "                                                 top_p=.9,\n",
    "                                                 num_beams=10,\n",
    "                                                 max_length=64,\n",
    "                                                 min_length=8,\n",
    "                                                 no_repeat_ngram_size=3,\n",
    "                                                 expert_alpha = 1.0,\n",
    "                                                 num_return_sequences=10\n",
    "                                                 )\n",
    "\n",
    "    # save all the answers with their associated scores in a df\n",
    "    answers = [bart_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in output_ids]\n",
    "    df_answers = pd.DataFrame(zip(answers, seq_scores), columns=['response', 'scores'])\n",
    "    df_answers['model-type'] = 'DExpert-article'\n",
    "\n",
    "    # save information about the question\n",
    "    df_answers['question'] = row['question']\n",
    "    df_answers['category'] = row['category']\n",
    "    df_answers['first_sentence'] = row['first_sentence']\n",
    "\n",
    "    # append the df\n",
    "    article_answer_df_lists.append(df_answers)\n",
    "\n",
    "\n",
    "\n",
    "DExperts_journal_test_answers = pd.concat(journal_answer_df_lists)\n",
    "DExperts_article_test_answers = pd.concat(article_answer_df_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DExperts_journal_test_answers = pd.concat(journal_answer_df_lists)\n",
    "DExperts_journal_test_answers.to_csv('{}/model_answers/DExperts_journal_test_answers.csv'.format(DATA_DIR))\n",
    "DExperts_article_test_answers.to_csv('{}/model_answers/DExperts_article_test_answers.csv'.format(DATA_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeDi\n",
    "\n",
    "\n",
    "This has to be run twice, one for each of the arg sets (journal or news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for namespacing the args\n",
    "class Bunch(object):\n",
    "    def __init__(self, adict):\n",
    "        self.__dict__.update(adict)\n",
    "           \n",
    "args = {'model_type': 'bart',\n",
    "        'gedi_model_type': 'bart',\n",
    "        'gen_model_name_or_path': '{}/bart_medq_wiki_s2orc/default/'.format(MODEL_DIR),\n",
    "        'gedi_model_name_or_path': '/homes/gws/taugust/Projects/ARK/sci_comm/resources/GeDi/topic_GeDi_journal_news_bart_medq_wiki',\n",
    "        'fp16': True, 'load_in_half_prec': False,\n",
    "        'config_name': '', 'tokenizer_name': '',\n",
    "        'cache_dir': '', 'do_lower_case': False,\n",
    "        'no_cuda': False, 'gen_length': 64,\n",
    "        'stop_token': None, 'temperature': 1.0,\n",
    "        'disc_weight': 30.0, 'filter_p': 0.8, \n",
    "        'class_bias': None, 'target_p': 0.8,\n",
    "        'do_sample': True, 'repetition_penalty': 1.2,\n",
    "        'rep_penalty_scale': 10.0, 'penalize_cond': True,\n",
    "        'k': 50, 'p': 0.9, 'gen_type': 'gedi',\n",
    "        'mode': 'topic', 'secondary_code': 'journal', \n",
    "        'gpt3_api_key': None, \n",
    "        'prompt': None,\n",
    "        'num_return_sequences':10,\n",
    "        'device':DEVICE}\n",
    "\n",
    "args = Bunch(args_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_df_lists_gedi_rows = []\n",
    "\n",
    "for i, r in tqdm(df_test.iterrows()):\n",
    "      \n",
    "    row = r.to_dict()                \n",
    "    \n",
    "    args.prompt = row['q_s2orc_doc']\n",
    "    args.term_start = None\n",
    "    \n",
    "    returned_answers = generate_GeDi.generate_GeDi_simple(args, tokenizer, model, gedi_model)\n",
    "    \n",
    "    # handle all the returned sequences\n",
    "    for answer, output in returned_answers:\n",
    "        new_row = row.copy()\n",
    "        new_row['response'] = answer.strip('</s>')\n",
    "        new_row['output'] = output[1].cpu().numpy()[0]\n",
    "        answer_df_lists_gedi_rows.append(new_row)\n",
    "\n",
    "    \n",
    "### make the df -- curently for journal\n",
    "bart_gedi_journal_test_answers = pd.DataFrame(answer_df_lists_gedi_rows)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving (change to news when running that one)\n",
    "bart_gedi_journal_test_answers.to_csv('{}/model_answers/bart_gedi_journal_test_answers.csv'.format(DATA_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining\n",
    "\n",
    "Combine all the answers into one df for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Here taking the top 10, for the rest take all of them \n",
    "\n",
    "### Reranking SVM\n",
    "bart_test_answers_svm_journal = bart_test_answers_svm.groupby(['question']).apply(lambda g: utils.get_top(g, col='predictions', n=10)).reset_index(drop=True)\n",
    "bart_test_answers_svm_news = bart_test_answers_svm.groupby(['question']).apply(lambda g: utils.get_top(g, col='predictions', ascending=True, n=10)).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make sure all responses have model type\n",
    "\n",
    "### Rerankers\n",
    "bart_test_answers_svm_journal['model-type'] = 'svm-rerank-journal'\n",
    "bart_test_answers_svm_news['model-type'] = 'svm-rerank-news'\n",
    "\n",
    "### Gedi\n",
    "bart_gedi_news_test_answers['model-type'] = 'gedi-news'\n",
    "bart_gedi_journal_test_answers['model-type'] = 'gedi-journal'\n",
    "\n",
    "\n",
    "### DExperts \n",
    "DExperts_news_test_answers = DExperts_article_test_answers\n",
    "DExperts_journal_test_answers['model-type'] = 'DExpert-journal'\n",
    "DExperts_news_test_answers['model-type'] = 'DExpert-news'\n",
    "\n",
    "\n",
    "### PPLM\n",
    "bart_pplm_journal_test_answers['model-type'] = 'PPLM-journal'\n",
    "bart_pplm_news_test_answers['model-type'] = 'PPLM-news'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = ['response', 'model-type', 'question', 'first_sentence', 'category']\n",
    "\n",
    "dfs = [bart_test_answers_bert_journal, bart_test_answers_bert_news,\n",
    "       bart_test_answers_svm_journal, bart_test_answers_svm_news,\n",
    "       bart_gedi_journal_test_answers, bart_gedi_news_test_answers,\n",
    "       DExperts_journal_test_answers, DExperts_news_test_answers,\n",
    "       bart_journals_test_answers, bart_articles_test_answers,\n",
    "      bart_pplm_journal_test_answers, bart_pplm_news_test_answers]\n",
    "\n",
    "df = pd.concat([d[common_columns] for d in dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('{}/model_answers/test_complexity_responses.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Measures\n",
    "\n",
    "Includes FK by concatting all definitions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability import Readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('{}/model_answers/test_complexity_responses.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['word_count'] > 0] \n",
    "df = jargon_utils.make_jargon_features(df, text_col='response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flesch Kincaid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### function for getting the overall Flesch Kincaid score by concating all definitions in a df together \n",
    "def get_overall_fk(df, def_col='response'):\n",
    "    all_defs = df[def_col].str.cat(sep=' ') \n",
    "    all_defs_readability = Readability(all_defs)\n",
    "    return all_defs_readability.flesch_kincaid()\n",
    "\n",
    "\n",
    "df.groupby(['model-type']).apply(lambda g: get_overall_fk(g))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
