{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition Modeling Expirements\n",
    "\n",
    "Notebook for what I imagine the first set of expirements will be. It will consist of testing a few different basic modeling approaches.  \n",
    "\n",
    "1) **Sequence-to-Sequence**: Includes finetuned BART (question: {} context: {}) setup. \n",
    "\n",
    "2) **Language Modeling**: Includes off-the-shelf few-shot GPT2 and GPT3 (from the paper: ```We append two held-out term and definition pairs, along with the first abstract drawn from each supporting document (we use the first abstract only to keep the full input within the modelsâ€™ context windows)```. Also finetuned GPT2 (with ```<question> term q <context> context doc <definition> definition``` tags).\n",
    "\n",
    "3) **Informational Retrieval**: An extractive approach over the support docs, using BiDAF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "     AutoTokenizer,\n",
    "     AutoModelForSeq2SeqLM,\n",
    "     AutoConfig, \n",
    "     AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "from importlib import reload  \n",
    "from joblib import dump, load\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nlp\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import  word_tokenize, sent_tokenize\n",
    "import random\n",
    "import uuid\n",
    "import spacy \n",
    "from tqdm import tqdm\n",
    "from spacy.pipeline import Sentencizer\n",
    "import re\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "sys.path.append('/lib')\n",
    "import utils\n",
    "\n",
    "MODEL_DIR = ''\n",
    "RESOURCE_DIR = ''\n",
    "DATA_DIR = ''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Dataset reloading - Ignore all of the above\n",
    "df_wiki_medq_strat_train = pd.read_csv('{}/model_data/train.csv'.format(DATA_DIR))\n",
    "df_wiki_medq_strat_dev = pd.read_csv('{}/model_data/dev.csv'.format(DATA_DIR))\n",
    "df_wiki_medq_strat_test = pd.read_csv('{}/model_data/test.csv'.format(DATA_DIR))\n",
    "\n",
    "\n",
    "df_medq_wiki = pd.concat([df_wiki_medq_strat_train, df_wiki_medq_strat_dev, df_wiki_medq_strat_test])\n",
    "\n",
    "# we use the dev set for testing the base generators, and reserve our test set for \n",
    "# complexity control, for training the generators we use splits of the train set\n",
    "df_test = df_wiki_medq_strat_dev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUMBER = ''\n",
    "CUDA_DEVICE = ''\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_NUMBER\n",
    "\n",
    "DEVICE = 'cuda:0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# function for wrapping the rather long code for generating answers across a df\n",
    "def generate_answers(df, model, tokenizer, model_type,\n",
    "                     num_return_sequences=1,\n",
    "                     num_beams=5,\n",
    "                     max_length=64,\n",
    "                     min_length=8,\n",
    "                     early_stopping=True,\n",
    "                     temperature=None,\n",
    "                     do_sample=True,\n",
    "                     top_k=50,\n",
    "                     top_p=0.9,\n",
    "                     max_input_length=1024,\n",
    "                     no_repeat_ngram_size=3,\n",
    "                     device=None):\n",
    "    \n",
    "    answer_df_lists = []\n",
    "    for i, r in tqdm(df.iterrows()):\n",
    "        \n",
    "        # set, or reset lengths (mostly for gpt2)\n",
    "        current_max_input = max_input_length\n",
    "        max_output_length = max_length\n",
    "        min_output_length = min_length\n",
    "        \n",
    "        row = r.to_dict()\n",
    "\n",
    "        # make the input, doing truncation differently depending on if it is bart or gpt2\n",
    "        if 'bart' in model_type.lower():\n",
    "            \n",
    "            # for bart, just truncate to max input length\n",
    "            q_doc = row['q_s2orc_doc'] \n",
    "            inputs = tokenizer(q_doc, return_tensors='pt', max_length=current_max_input, truncation=True)\n",
    "            \n",
    "                \n",
    "        elif 'gpt'in model_type.lower():\n",
    "            # for gpt2 truncate from the front to preserve the question at the end.\n",
    "            q_doc = r['gpt2_task_row']\n",
    "            \n",
    "            # make space for input based on max output allowed  \n",
    "            current_max_input -= max_output_length \n",
    "            \n",
    "            inputs = tokenizer(q_doc, return_tensors='pt', truncation=False) \n",
    "            input_length = inputs['input_ids'].shape[1]\n",
    "            if input_length > current_max_input:\n",
    "                print('Input length of {} is too long for max input of {}, truncating to {}.'.format(input_length, current_max_input, current_max_input))\n",
    "                # this is truncating from the back\n",
    "                inputs['input_ids'] = inputs['input_ids'][:, -current_max_input:]\n",
    "                inputs['attention_mask'] = inputs['attention_mask'][:, -current_max_input:]\n",
    "                input_length = current_max_input\n",
    "                \n",
    "        else:\n",
    "            raise Exception('Model type {} not found!'.format(model_type))\n",
    "            \n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "        \n",
    "        # resize max length based on the size of the input for gpt models\n",
    "        if 'gpt'in model_type.lower():\n",
    "            max_output_length += input_length\n",
    "            min_output_length += input_length\n",
    "            if 'ootb' not in model_type.lower():\n",
    "                # also set the special end token (if this is a finetuned gpt model) to the context token\n",
    "                eos_token_id = tokenizer.additional_special_tokens_ids[1]\n",
    "                \n",
    "        outputs = model.generate(**inputs.to(device),\n",
    "                                 decoder_start_token_id=tokenizer.bos_token_id,\n",
    "                                 num_return_sequences=num_return_sequences,\n",
    "                                 num_beams=num_beams, \n",
    "                                 min_length=min_output_length,\n",
    "                                 max_length=max_output_length, \n",
    "                                 early_stopping=early_stopping, \n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=do_sample,\n",
    "                                 top_k=top_k,\n",
    "                                 top_p=top_p,\n",
    "                                 eos_token_id=eos_token_id,\n",
    "                                 no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                 output_scores=True,\n",
    "                                 return_dict_in_generate=True)\n",
    "\n",
    "        # save all the answers with their associated scores in a df\n",
    "        # because GPT2 includes what the answer was conditioned on, we have to strip that\n",
    "        if 'gpt'in model_type.lower():\n",
    "            answers = [tokenizer.decode(ans_ids[input_length:], skip_special_tokens=True).strip() for ans_ids in outputs[0]]\n",
    "        else:\n",
    "            answers = [tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in outputs[0]]\n",
    "        \n",
    "        df_answers = pd.DataFrame(zip(answers, outputs['sequences_scores'].tolist()), columns=['response', 'scores'])\n",
    "        \n",
    "        df_answers['model-type'] = model_type\n",
    "        \n",
    "        # save information about the question\n",
    "        df_answers['question'] = row['question']\n",
    "        df_answers['category'] = row['category']\n",
    "        df_answers['first_sentence'] = row['first_sentence']\n",
    "    \n",
    "        # append the df\n",
    "        answer_df_lists.append(df_answers)\n",
    "        \n",
    "    return pd.concat(answer_df_lists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bart model - finetuned on medq s2orc\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"{}/bart_medq_wiki_gen\".format(MODEL_DIR))\n",
    "bart_model = AutoModelForSeq2SeqLM.from_pretrained(\"{}/bart_medq_wiki_gen\".format(MODEL_DIR))\n",
    "_ = bart_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_answers = generate_answers(df=df_test, model=bart_model, tokenizer=bart_tokenizer, model_type='bart', use_decoder_prefix=False, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(bart_answers[['question', 'first_sentence', 'response', 'scores']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the bart model off the gpu\n",
    "bart_model=bart_model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuned GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['gpt2_task_row'] = [utils.make_gpt_doc(r['question'], r['support_doc_sparse_s2orc'], None, testing=True) for _,r in df_test.iterrows()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 model - finetuned on medq and s2orc\n",
    "finetuned_gpt2_model_path = '/homes/gws/taugust/Projects/ARK/sci_comm/models/gpt2_full_gen'\n",
    "\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(finetuned_gpt2_model_path).to(DEVICE)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(finetuned_gpt2_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_answers = generate_answers(df=df_test, model=gpt2_model, tokenizer=gpt2_tokenizer, model_type='gpt2', device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(gpt2_dev_answers[['question', 'first_sentence', 'response', 'scores']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model=gpt2_model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOTB GPT2\n",
    "\n",
    "Right now we are using the first two rows as a few shot setting, so MAKE SURE YOU TAKE THEM OUT IN SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input should be the context doc + 2 new lines + a templated question\n",
    "\n",
    "def make_templated_input(support_doc, question):\n",
    "    return '{}\\n\\n{}'.format(support_doc, question)\n",
    "\n",
    "\n",
    "def make_few_shot_templated_input(support_doc, question, few_shot_prefix):\n",
    "    return '{}\\n {}\\n\\n{}'.format(few_shot_prefix, support_doc, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Ignoring the support doc\n",
    "\n",
    "def make_qa_prompt(q, definition):\n",
    "    return 'question: {} \\ndefinition: {}'.format(q, definition)\n",
    "\n",
    "def make_task_row(few_shot_prefix, row):\n",
    "    return '{}{}'.format(qa_few_shot_prefix, make_qa_prompt(row['question'], ''))\n",
    "\n",
    "\n",
    "row1=df_wiki_medq_strat_dev.iloc[0]\n",
    "row2=df_wiki_medq_strat_dev.iloc[1]\n",
    "\n",
    "\n",
    "ex1 = make_qa_prompt(row1['question'], row1['first_sentence'])\n",
    "ex2 = make_qa_prompt(row2['question'], row2['first_sentence'])\n",
    "\n",
    "qa_few_shot_prefix = '{}\\n\\n###\\n\\n{}\\n\\n###\\n\\n'.format(ex1, ex2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this work with the above function, just remake the task row\n",
    "df_test['gpt2_task_row'] = [make_task_row(qa_few_shot_prefix, r) for _,r in df_test.iterrows()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ootb_gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2-medium').to(DEVICE)\n",
    "ootb_gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ootb_gpt2_answers = generate_answers(df=df_test[2:], model=ootb_gpt2_model, tokenizer=ootb_gpt2_tokenizer, model_type='ootb-gpt2', device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(ootb_gpt2_dev_answers[['question', 'first_sentence', 'generated_def', 'scores']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the end move the model off the gpu (if it was there)\n",
    "ootb_gpt2_model=ootb_gpt2_model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOTB GPT3 - maxed out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "ENV_DIR = ''\n",
    "\n",
    "# get the api key \n",
    "with open('{}/vars.txt'.format(ENV_DIR), 'r') as f:\n",
    "    env_vars = f.readlines()\n",
    "    \n",
    "env_vars = [tuple(v.split('=')) for v in env_vars]\n",
    "assert(env_vars[0][0] == 'OPENAI_API_KEY')\n",
    "open_api_key = env_vars[0][1]\n",
    "openai.api_key = open_api_key.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from: https://beta.openai.com/docs/introduction/factual-responses\n",
    "def get_gpt3_output(openai, prompt, max_tokens=64):\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"davinci\",\n",
    "      prompt=prompt,\n",
    "      max_tokens=max_tokens,\n",
    "      top_p=0.9,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"###\"]\n",
    "    )\n",
    "    return response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '{}{}'.format(qa_few_shot_prefix, make_qa_prompt(test_row['question'], ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ootb_gpt3_answers = df_test.sample(100, random_state=84, replace=False).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### because gpt3 just gives back out answers, duplicate the df_dev and set the response column to the output\n",
    "ootb_gpt3_answers['model-type'] = 'ootb-gpt3'\n",
    "ootb_gpt3_answers['response'] = [get_gpt3_output(openai, p, max_tokens=64) for p in tqdm(ootb_gpt3_dev_answers['gpt2_task_row'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ootb_gpt3_dev_answers = df[df['model-type'] == 'ootb-gpt3']\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(ootb_gpt3_answers[['question', 'first_sentence', 'response']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating \n",
    "\n",
    "Doing ROUGE and BERTscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_metrics, load_metric\n",
    "\n",
    "bert_score = load_metric(\"bertscore\")\n",
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge(df, rouge_metric, ref_col, response_col):\n",
    "    scores = rouge_metric.compute(\n",
    "        predictions=df[response_col], references=df[ref_col],\n",
    "        rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
    "        use_agregator=True, use_stemmer=False\n",
    "    )\n",
    "    rows = []\n",
    "    measure_mapping = {'F':lambda x: x.mid.fmeasure}\n",
    "\n",
    "    for t in scores.keys():\n",
    "        for measure in ['F']:\n",
    "            rows.append({'score_type':t, 'score_measure':measure, 'score':measure_mapping[measure](scores[t])})\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([ootb_gpt2_answers, gpt2_answers, bart_answers, df_extracted_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['model-type']).apply(lambda g: calc_rouge(g, rouge, ref_col='first_sentence', response_col='response'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = bert_score.compute(\n",
    "        predictions=df['response'], references=df['first_sentence'], lang='en', verbose=False, device=None)\n",
    "\n",
    "df['bert_scores'] = scores['f1']\n",
    "df.groupby(['model-type'])['bert_scores'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
